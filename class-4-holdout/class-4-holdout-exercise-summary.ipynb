{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDELdQ-j0lDB"
      },
      "source": [
        "# **Programa de Pós-Graduação em Computação - INF/UFRGS**\n",
        "\n",
        "\n",
        "\n",
        "### Disciplina CMP263 - Aprendizagem de Máquina\n",
        "#### *Aluno: Henrique Krausburg Correa (henrique@inf.ufrgs.br)*\n",
        "#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n",
        "<br>\n",
        "\n",
        "***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas  e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de algoritmos e estratégias de avaliação em Aprendizado de Máquina.*\n",
        "\n",
        "\n",
        "\n",
        "# **Tópico: Introdução à avaliação de modelos com Holdout**\n",
        "\n",
        "\n",
        "**Objetivos da atividade:**\n",
        "-  Entender o funcionamento da técnica de holdout para avaliação de modelos.\n",
        "- Comparar o desempenho de dois algoritmos, Naive Bayes e KNN, em uma tarefa de classificação.\n",
        "- Aplicar métricas de avaliação como acurácia, precisão, recall, observando a matriz de confusão.\n",
        "- Implementar uma estratégia básica para seleção de hiperparâmetros para o KNN, interpretando os resultados.\n",
        "- Analisar o impacto de aspectos como dimensão dos dados, aleatoriedade na divisão de dados e repetições na avaliação de modelos com Holdout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xLyeKJl1Clw"
      },
      "source": [
        "## Fazendo a divisão dos dados com Holdout de 3 vias (treino/validação/teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 1. Considere um cenário com 5 instâncias no conjunto de treinamento e  95 instâncias no conjunto de teste. O quão boa você acha que é a capacidade de generalização do modelo que provavelmente treinaremos?\n",
        "\n",
        "Péssima. A qualidade das predições está relácionado (mas não limitado) a:\n",
        "- Quantidades de instâncias no conjunto de treinamento (quanto mais, melhor)\n",
        "- Quantidade de atributos\n",
        "- Complexidade do algorítmo\n",
        "- Distribuição das classes nos conjuntos de treino e teste\n",
        "\n",
        "Em um conjunto de treinamento pequeno, teríamos um dos extremos:\n",
        "- Underfitting: Não tem dados suficientes para o modelo aprender a resolver o problema\n",
        "- Overfitting: O treino resume-se a memorizar ruído. Incapaz de generalizar para novos casos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 2. Sua resposta para 1 muda se tivermos 500 instâncias de treinamento e 9500 instâncias de teste?\n",
        "\n",
        "Certamente. O que interessa é a quantidade de dados necessários para resolver um problema. Adotar uma proporção entre instância de treinamento e teste sem um critério definido não produz benefícios.\n",
        "\n",
        "Caso de uso:\n",
        "- Utiliza-se um algorítmo simples para predição\n",
        "- Generaliza rápido com poucas instâncias (logarítmico)\n",
        "- Deseja-se uma forte avaliação de desempenho do modelo quando o erro \"custa caro\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 3.  Considere um cenário com 95 instâncias no conjunto de treinamento e 5 instâncias no conjunto de teste. O valor de desempenho do teste ainda é uma boa estimativa do poder de generalização?\n",
        "\n",
        "Não, pois haveria chance de todas as instâncias conter dados repetidos do conjunto de treino. Isto prejudica avaliar o poder de generalização.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 4. Sua resposta para 3 muda se tivermos 9500 instâncias de treinamento e 500 instâncias de teste?\n",
        "\n",
        "Sim. Esta divisão pode ser adequada para cenários de poucas classes que sejam fáceis de classificar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZITv0JD1JMH"
      },
      "source": [
        "## Pré-processamento: Normalizando os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Com base nos resultados, qual dos dois modelos apresentou melhor desempenho geral no conjunto de teste, Naïve Bayes ou kNN? Em quais classes o kNN obteve melhor sensibilidade (recall) e precisão do que o Naive Bayes, ou vice versa?\n",
        "\n",
        "O modelo KNN em que K=5 obteve melhor precisão e recall ao inferir a classe 1 (Benigno) em relação ao Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvQItjrUD8C8"
      },
      "source": [
        "## Analisando o impacto da divisão aleatória de dados no desempenho dos modelos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Observe a variação do valor de *random_state* na divisão dos dados e os respectivos resultados do desempenho na classificação. Como isso afeta os resultados? Explique o impacto de diferentes divisões dos dados de treino/validação/teste no desempenho dos modelos.\n",
        "\n",
        "O exemplo demonstra que o treino de um modelo está sujeito à vies introduzido por amostragem de dados. Diferentes ***random_states*** podem gerar divisões com distribuições de classes ou características distintas. Isto pode favorecer ou penalizar o modelo dependendo da sua \"sorte\" na amostragem. Os algorítmos podem apresentar uma progressão de aprendizado:\n",
        "\n",
        "- Gulosa: Rápido aprendizado nas primeiras instâncias. Podem estabilizar cedo demais e deixam de explorar oportunidades de melhoria.\n",
        "- Conservadora: Demoram para convergir para um resultado satisfatório. Demanda mais instâncias para treinar. Busca por um resultado ótimo.\n",
        "\n",
        "Portanto, um modelo pode obter um excelente resultado na validação, porém um desempenho inferior no teste final. \n",
        "\n",
        "> Por que é importante repetir os experimentos várias vezes, variando o random_state? O que a repetição traz em termos de confiabilidade dos resultados?\n",
        "\n",
        "Ao repetir experimentos, embaralhando as condições iniciais dos conjuntos de dados, estamos testando a maturidade do processo de desenvolvimento utilizado para construir o modelo. Isto responde questionamentos como:\n",
        "\n",
        "- Escolhemos o algorítmo correto para o problema de domínio?\n",
        "- O quão robusto é o modelo frente ao viés no conjunto de dados?\n",
        "- Precisamos realizar algum ajuste / mitigação no modelo?\n",
        "\n",
        "Repetir os experimentos permite obter uma estimativa mais confiável do desempenho médio do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZF7RT_jEm_B"
      },
      "source": [
        "## Analisando o impacto do tamanho do conjunto de teste na avaliação de desempenho dos modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Conforme o tamanho do conjunto de teste aumenta, como muda a variância no desempenho do modelo? Por que esse comportamento ocorre? \n",
        "\n",
        "A variância no desempenho do modelo diminuí a medida que o tamanho do conjunto de teste aumenta. Isto ocorre devido a quantidade de instâncias disponíveis, permitindo testar a capacidade de generalização do modelo para dados desconhecidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Ao comparar a amplitude (diferença entre o máximo e o mínimo) do desempenho em diferentes tamanhos de conjunto de teste, o que você observa? \n",
        "\n",
        "A amplitude tende a ser maior em conjuntos de testes pequenos, menor em conjuntos grandes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Qual é a relação entre o tamanho do teste e a amplitude dos resultados?\n",
        "\n",
        "A amplitude dos resultados é inversamente proporcional ao tamanho do teste. Baixa amplitude e variância representam estabilidade e confiança na avaliação de desempenho do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Se a variância dos resultados de acurácia for muito alta, o que isso pode indicar sobre o seu modelo ou sobre a forma como os dados estão sendo divididos?\n",
        "\n",
        "Significa que o modelo possui ótimo desempenho para alguns conjuntos de teste, porém péssimo para outros. São sinais que indicam:\n",
        "\n",
        "- Poucos dados para avaliar o desempenho do modelo\n",
        "- Sensiblidade do modelo ao viés de particionamento\n",
        "- Potêncial desbalanceamento de classe no particionamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV7YEZf_Eu39"
      },
      "source": [
        "## Analisando o impacto do tamanho do conjunto de treino na avaliação de desempenho dos modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Mantendo o conjunto de teste fixo, que mudança ou tendência observamos no desempenho dos modelos conforme mais dados de treino são utilizados?\n",
        "\n",
        "Observa-se que o desempenho aumenta proporcional a quantidade de dados adicionadas ao conjunto de treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Como a amplitude (diferença entre o máximo e o mínimo) do desempenho varia com o tamanho do conjunto de treino?\n",
        "\n",
        "Tanto a amplitude quanto a variância diminuem ao passo que o conjunto de treino aumenta, sinalizando a estabilização do sistema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> O que isso nos diz sobre a confiabilidade do modelo com tamanhos pequenos de conjunto de treino?\n",
        "\n",
        "Baixa confiabilidade para modelos com poucas instâncias de treino disponíveis. O resultado é um underfit, pois o modelo não conseguiu aprender a resolver o problema de domínio e precisa de mais amostras para melhorar sua precisão.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
